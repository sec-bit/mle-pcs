{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ea4144-011a-43db-930e-30300372da89",
   "metadata": {},
   "source": [
    "# Shannon's Theorem and Probabilistic Noise\n",
    "\n",
    "So far, we have looked at codes from the **Hamming perspective**, where we assume a \"worst-case\" scenario: up to a certain number of errors (*t*) will occur, and our code must be able to handle it. This is an adversarial model.\n",
    "\n",
    "This notebook introduces a different perspective. Instead of a worst-case scenario, Shannon modeled noise **stochastically**, or **probabilistically**. He viewed communication as a random process where errors happen with a certain likelihood, not with a guarantee.\n",
    "\n",
    "## The Communication Process\n",
    "\n",
    "Shannon's framework separates the problem of communication into two distinct parts, as shown in the general model below.\n",
    "\n",
    "<img src=\"./imgs/image_channel_diagram.png\" alt=\"Generic model of a communication system\" width=\"600\"/>\n",
    "\n",
    "1. **Source Coding (Compression):** This part deals with the message itself before noise is a factor. The goal of the **Source Encoder** is to remove redundancy from the original data to make it as compact as possible. The theoretical limit of this compression is related to the **entropy** of the source. Think of this as zipping a file.\n",
    "\n",
    "2. **Channel Coding (Error Correction):** This is the part we've been focused on. After the message is compressed, the **Channel Encoder** adds \"smart\" redundancy back in. This redundancy is not a simple repetition; it's structured (like in an RS code) to protect the data from errors that occur when it's sent over the noisy **Channel**.\n",
    "\n",
    "Shannon showed that these two problems can be studied separately. For the rest of this chapter, we will ignore source coding and focus exclusively on **channel coding** in a world where the noise is random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4767e17-b9df-413a-9f3f-32651a669e8a",
   "metadata": {},
   "source": [
    "## Shannon's Noise Models: From BSC to Channel Capacity\n",
    "\n",
    "Shannon's framework begins with a formal model of a noisy channel. It consists of three key parts:\n",
    "\n",
    "1. **Input alphabet** $\\,\\mathcal{X}\\,$ — the set of symbols we can send.\n",
    "2. **Output alphabet** $\\,\\mathcal{Y}\\,$ — the set of symbols we might receive.\n",
    "3. **Transition matrix** $\\,M\\,$ — contains crossover probabilities. The entry $M(x,y)$ is the probability of receiving symbol $y$ given that we sent symbol $x$, written as $\\Pr(y \\mid x)$.\n",
    "\n",
    "Channels are typically assumed to be **memoryless**, meaning the noise on each transmitted symbol is an independent event.\n",
    "\n",
    "### Common Stochastic Channels\n",
    "\n",
    "Let's look at a specific channel model.\n",
    "\n",
    "#### Binary Symmetric Channel (BSC)\n",
    "\n",
    "This is the most fundamental model. The input and output alphabets are both binary:\n",
    "$\\,\\mathcal{X} = \\mathcal{Y} = \\{0,1\\}.$\n",
    "\n",
    "- A bit is transmitted correctly (0 stays 0, 1 stays 1) with probability $1 - p$.\n",
    "- A bit is flipped (0 becomes 1, 1 becomes 0) with probability $p$. This $p$ is the **crossover probability**.\n",
    "\n",
    "The channel can be visualized as:\n",
    "\n",
    "<img src=\"./imgs/image_bsc.png\" alt=\"Binary Symmetric Channel Diagram\" width=\"300\"/>\n",
    "\n",
    "The corresponding transition matrix is:\n",
    "\n",
    "$$\n",
    "M = \\begin{pmatrix}\n",
    "1 - p & p \\\\\n",
    "p & 1 - p\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0204840a-fa07-41d1-a5bd-d04012b72afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original codeword: [0 1 0 1 1 1 0]\n",
      "Received codeword: [0 1 1 1 1 0 0] (after BSC with p=0.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simulate_bsc(codeword, p):\n",
    "    \"\"\"Simulates passing a codeword through a Binary Symmetric Channel.\"\"\"\n",
    "    received_word = []\n",
    "    for bit in codeword:\n",
    "        if np.random.rand() < p:\n",
    "            # The bit is flipped\n",
    "            received_word.append(1 - bit)\n",
    "        else:\n",
    "            # The bit is transmitted correctly\n",
    "            received_word.append(bit)\n",
    "    return np.array(received_word)\n",
    "\n",
    "# Example\n",
    "original_codeword = np.array([0, 1, 0, 1, 1, 1, 0])\n",
    "crossover_prob = 0.1\n",
    "received_codeword = simulate_bsc(original_codeword, crossover_prob)\n",
    "\n",
    "print(f\"Original codeword: {original_codeword}\")\n",
    "print(f\"Received codeword: {received_codeword} (after BSC with p={crossover_prob})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286abb2-97db-436d-b67e-da53e694d52c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### q-ary Symmetric Channel (qSC)\n",
    "\n",
    "This model generalizes the BSC to an alphabet of any size $q \\ge 2$.\n",
    "\n",
    "- A symbol is transmitted correctly with probability $1 - p$.\n",
    "- If an error occurs, the original symbol is transformed into any of the other $q - 1$ incorrect symbols with equal probability $\\,\\frac{p}{q-1}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Binary Erasure Channel (BEC)\n",
    "\n",
    "In the BEC, bits are never flipped, but they can be completely lost or “erased”.\n",
    "\n",
    "- The input alphabet is $\\mathcal{X} = \\{0,1\\}$.\n",
    "- The output alphabet is $\\mathcal{Y} = \\{0,1,?\\}$, where “?” denotes an **erasure**.\n",
    "- A bit is transmitted correctly with probability $1 - a$.\n",
    "- A bit is erased with probability $a$.\n",
    "\n",
    "<img src=\"./imgs/image_bec.png\" alt=\"Binary Erasure Channel Diagram\" width=\"200\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### Binary Input Additive Gaussian White Noise (BIAGWN)\n",
    "\n",
    "This is a crucial model for **continuous** channels, like radio waves.\n",
    "\n",
    "- The input is binary, typically $\\mathcal{X}=\\{-1,1\\}$.\n",
    "- The output $\\mathcal{Y}$ is the set of all real numbers ($\\mathbb{R}$).\n",
    "- The channel adds random noise drawn from a Gaussian (Normal) distribution to the input.  \n",
    "  The probability density of receiving $y$ given $x$ is\n",
    "  $$\n",
    "  \\Pr(y \\mid x) \\;=\\; \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(y-x)^2}{2\\sigma^2}\\right).\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5826e-9d03-4e9d-bb4d-6823fa386fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c99fa0-1ca0-4361-8019-e9c331379fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
