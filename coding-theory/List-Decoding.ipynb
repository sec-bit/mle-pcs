{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac070197-374b-46d7-9427-05a55b9a78c1",
   "metadata": {},
   "source": [
    "## List Decoding: Bridging the gap between Hamming and Shannon\n",
    "\n",
    "### Two Perspectives on Noise: Hamming vs. Shannon\n",
    "\n",
    "In coding theory, we have two fundamental views on communication over a noisy channel:\n",
    "\n",
    "- **Shannon theory.** This probabilistic model shows that for a channel with random noise (like the q-ary Symmetric Channel, or qSC$_p$), we can achieve reliable communication for any rate  \n",
    "  $$R < 1 - H_q(p).$$\n",
    "  This promises successful communication even with a relatively high fraction of errors.\n",
    "\n",
    "- **Hamming theory.** This model takes a more pessimistic, worst-case view. It provides a 100% guarantee of error correction, but only for a much smaller fraction of errors.\n",
    "\n",
    "There is a significant gap between the number of errors these two theories can handle. This section explores the mathematical and geometric reasons for the strict limits of the Hamming world, which sets the stage for list decoding as a bridge between the two.\n",
    "\n",
    "\n",
    "### The Quantitative Limit of Unique Decoding\n",
    "\n",
    "Let’s define our terms from the Hamming perspective. A code has a **rate** $R = k/n$ (message length / codeword length) and a **relative distance** $\\delta = d/n$ (minimum distance / codeword length).\n",
    "\n",
    "The central tenet of unique decoding is that it can correct a fraction of errors up to half the relative distance. By the Singleton bound, we know that a code’s relative distance is limited by its rate:\n",
    "$$\\delta \\le 1 - R.$$\n",
    "This directly implies that the fraction of correctable errors, $p$, must satisfy the following condition:\n",
    "$$p \\le \\frac{1 - R}{2}.$$\n",
    "\n",
    "This is the hard barrier for unique decoding. If the fraction of errors exceeds this, the worst-case guarantee is broken. Reed–Solomon codes are optimal in this regard, as they can achieve this bound.\n",
    "\n",
    "\n",
    "### Visualizing the Breakdown: The “Bad Examples”\n",
    "\n",
    "The reason for this strict limit is best understood visually. Consider the figure\n",
    "<img src=\"./imgs/image_bad_examples.png\" alt=\"Generic model of a communication system\" width=\"600\"/>\n",
    "\n",
    "where $c_1, c_2, c_3, c_4$ are valid codewords.\n",
    "\n",
    "The decoder fails for received words that fall into the “bad examples” region (the area with dotted lines). This happens in two key scenarios:\n",
    "\n",
    "- **Ambiguity (point $y$).** The received word $y$ has been corrupted such that it lies exactly halfway between two codewords, say $c_1$ and $c_4$. Its distance to both is exactly $\\delta/2$. Since there is no **unique** closest codeword, the decoder must give up.\n",
    "\n",
    "- **Decoding failure (point $z$).** The received word $z$ does not fall within the $\\delta/2$ decoding radius of **any** codeword. It exists in the interstitial space between them, and the decoder again declares a failure.\n",
    "\n",
    "\n",
    "### The Path Forward: List Decoding\n",
    "\n",
    "The unique decoding model is pessimistic because the number of these “bad examples” is insignificant compared to the total volume of possible received words. However, the model’s strict requirement for a unique answer forces it to fail even in these rare cases.\n",
    "\n",
    "To overcome this, we relax the demand for a single candidate. This leads to **list decoding**, a paradigm where the decoder, instead of failing, returns a short list of all plausible codewords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0028c15-7304-4b20-8271-a26e1c5a0941",
   "metadata": {},
   "source": [
    "## Notebook Section 2: The Core Concept of List Decoding\n",
    "\n",
    "### A New Paradigm: From One to Many\n",
    "\n",
    "In the previous section, we saw how the strict requirement of a single, unique answer forced the decoder to fail even in scenarios where a received word was close to a small number of valid codewords. To overcome this limitation, we turn to a relaxed notion of decoding called **list decoding**.\n",
    "\n",
    "Instead of outputting a single candidate for the message, a list-decoding algorithm is allowed to output a short list of all plausible messages. This notion is formally parameterized by two values:\n",
    "\n",
    "- **$\\rho$** (rho): The fraction of errors we wish to correct. This defines the radius of our search.\n",
    "- **$L$**: A number representing the maximum allowed size of the output list.\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "The concept of list decodability is fundamentally a combinatorial property of a code. It guarantees that no single point in the entire space of possible received words is “too close” to a large number of codewords simultaneously. The formal definition is as follows:\n",
    "\n",
    "**Combinatorial List Decoding:**  \n",
    "Given $0 \\le \\rho \\le 1$ and $L \\ge 1$, a code $C \\subseteq \\Sigma^n$ is $(\\rho, L)$-list decodable if for every received word $y \\in \\Sigma^n$, we have:\n",
    "$$\n",
    "\\left|\\{\\, c \\in C \\mid \\Delta(y, c) \\le \\rho n \\,\\}\\right| \\le L .\n",
    "$$\n",
    "\n",
    "Let’s break this down:\n",
    "\n",
    "- This is a worst-case definition that must hold for **every** possible received word $y$.\n",
    "- $\\Delta(y, c)$ is the Hamming distance between the received word $y$ and a codeword $c$.\n",
    "- The set $\\{\\ldots\\}$ contains all codewords from the code $C$ that are inside a Hamming ball of radius $\\rho n$ centered at $y$.\n",
    "- The definition simply states that the size of this set can never be larger than $L$.\n",
    "\n",
    "### The List-Decoding Algorithm and Guarantee\n",
    "\n",
    "While the definition above is about the code’s structure, a list-decoding **algorithm** works as follows: given an error parameter $\\rho$, a code $C$, and a received word $y$, the algorithm’s task is to find and output **all** codewords that are within a relative Hamming distance $\\rho$ of $y$.\n",
    "\n",
    "This provides a powerful guarantee:\n",
    "\n",
    "> **If the fraction of errors that actually occurred during transmission was at most $\\rho$, then the transmitted codeword is guaranteed to be in the algorithm’s output list.**\n",
    "\n",
    "Of course, the choice of $L$ is critical. If we set $L = 1$, we simply recover the notion of unique decoding. If we allow $L$ to be exponentially large, the concept becomes trivial. Therefore, our focus is on cases where $L$ is a small constant or, more generally, grows polynomially with the block length $n$.\n",
    "\n",
    "### Practical Utility \n",
    "\n",
    "A natural question arises: if the decoder returns a list with more than one item, how do we recover the single correct message? There are two primary approaches to this problem:\n",
    "\n",
    "1. **Declare a decoding error if the list size is greater than 1.**  \n",
    "   This still represents a significant gain over unique decoding. For most error patterns, the list size is one with high probability, meaning we can successfully decode many more error patterns than the strict $d/2$ limit allows.\n",
    "\n",
    "2. **Use side information to select the correct message.**  \n",
    "   If the decoder has access to some external information, it can use that to “prune” the list. Informally, to pick the correct item from a list of size $L$, one needs approximately $O(\\log L)$ extra bits of information. This is especially useful in applications like complexity theory, where maximizing the rate of the code is not the primary objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408c44c-9cb4-4568-8ef4-2601f0ccb96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
