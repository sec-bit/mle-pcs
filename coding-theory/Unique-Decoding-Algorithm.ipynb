{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e952d170-d198-4b0c-984b-669b85770795",
   "metadata": {},
   "source": [
    "# Guide to Unique Decoding of Reed-Solomon Codes\n",
    "\n",
    "This notebook provides the explanation and implementation of the **Welch-Berlekamp algorithm** for the unique decoding of Reed-Solomon codes. We will start by defining the unique decoding problem, develop the intuition behind the algorithm using a geometric perspective, and finally build and demonstrate a working Python implementation.\n",
    "\n",
    "## The Unique Decoding Problem\n",
    "\n",
    "A Reed-Solomon code is created by taking a message, treating it as the coefficients of a **message polynomial** $P(X)$ of degree less than $k$, and then evaluating that polynomial at $n$ distinct points $(\\alpha_1, \\dots, \\alpha_n)$ to create a codeword.\n",
    "\n",
    "When this codeword is transmitted, a noisy channel may introduce errors, resulting in a **received word** $y = (y_1, \\dots, y_n)$. The number of errors, $e$, is the number of positions where the received word differs from the original codeword.\n",
    "\n",
    "The goal of **unique decoding** is to recover the *one and only* original polynomial $P(X)$ from the noisy received word $y$. This is only guaranteed to be possible if the number of errors is strictly less than half the minimum distance of the code. For a Reed-Solomon code, this condition is:\n",
    "\n",
    "$$ e < \\frac{n-k+1}{2} $$\n",
    "\n",
    "This notebook will build the Welch-Berlekamp algorithm, an efficient method for solving this exact problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6436473-b1eb-4af8-99c9-d9b00b9ddeaa",
   "metadata": {},
   "source": [
    "## A Geometric View of the Problem \n",
    "\n",
    "To better visualize the problem, we can perform a \"syntactic shift\" and think of the received word $y$ not as a vector, but as a collection of $n$ points in a 2D plane: $\\{(\\alpha_1, y_1), (\\alpha_2, y_2), \\dots, (\\alpha_n, y_n)\\}$.\n",
    "\n",
    "<img src=\"./imgs/image_unique_decoding_1.png\" width=\"600\"/>\n",
    "\n",
    "The image above shows an example of a received word with n=14 points and k=2. The original message was a line (a polynomial of degree k-1=1).\n",
    "\n",
    "The original, uncorrupted codeword would consist of points that all lie perfectly on the curve defined by the message polynomial $P(X)$. The effect of noise is to knock some of these points off the curve. The decoder's job, in this geometric view, is to find the unique curve of degree less than $k$ that passes through the maximum number of these received points.\n",
    "\n",
    "<img src=\"./imgs/image_unique_decoding_2.png\"  width=\"600\"/>\n",
    "\n",
    "The above image illustrates this. The correct polynomial, $P(X)=X$, passes through the subset of \"correct\" points, while the \"error\" points lie scattered off the line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73a071-c311-47cc-963c-1fe87eab6a29",
   "metadata": {},
   "source": [
    "## The Core Idea: Reverse Engineering a Solution \n",
    "\n",
    "The Welch-Berlekamp algorithm is designed using a \"reverse engineering\" approach. We start by assuming we magically know the solution—both the original polynomial $P(X)$ and the locations of the errors—and derive a mathematical property. Then, we use that property to build an algorithm that finds $P(X)$ without knowing the error locations beforehand.\n",
    "\n",
    "### The Error-Locator Polynomial, E(X)\n",
    "\n",
    "Let's define a special tool called the **Error-Locator Polynomial**, $E(X)$. This is a polynomial whose roots are the x-coordinates ($\\alpha_i$) where an error occurred. In other words:\n",
    "\n",
    "$$ E(\\alpha_i) = 0 \\quad \\text{if} \\quad y_i \\neq P(\\alpha_i) $$\n",
    "\n",
    "If there are $e$ errors, we can construct such a polynomial of degree $e$.\n",
    "\n",
    "### The Key Equation\n",
    "\n",
    "With this definition, we can establish a key equation that holds true for **every single point**, whether it's an error or not:\n",
    "\n",
    "$$ y_i E(\\alpha_i) = P(\\alpha_i) E(\\alpha_i) \\quad \\text{for all } i=1, \\dots, n $$\n",
    "\n",
    "\n",
    "This powerful identity is easy to prove by considering two cases:\n",
    "1.  **If an error occurred at $\\alpha_i$**: By definition, $E(\\alpha_i) = 0$. The equation becomes $y_i \\cdot 0 = P(\\alpha_i) \\cdot 0$, which simplifies to $0=0$. The identity holds.\n",
    "2.  **If no error occurred at $\\alpha_i$**: In this case, we know $y_i = P(\\alpha_i)$. If we multiply both sides of this by $E(\\alpha_i)$, the equality is preserved. The identity also holds.\n",
    "\n",
    "This equation is the foundation of the algorithm. The only problem is that it involves the product of two unknowns, $P(X)$ and $E(X)$, making it a difficult quadratic problem to solve directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dff371-71a9-4941-9491-fec65e21e0db",
   "metadata": {},
   "source": [
    "## 3. The Core Idea: Reverse Engineering a Solution \n",
    "\n",
    "The Welch-Berlekamp algorithm is designed using a clever \"reverse engineering\" approach. We start by assuming we magically know the solution—both the original polynomial $P(X)$ and the locations of the errors—and derive a mathematical property. Then, we use that property to build an algorithm that finds $P(X)$ without knowing the error locations beforehand.\n",
    "\n",
    "### The Error-Locator Polynomial, E(X)\n",
    "\n",
    "Let's define a special tool called the **Error-Locator Polynomial**, $E(X)$. This is a polynomial whose roots are the x-coordinates ($\\alpha_i$) where an error occurred. In other words:\n",
    "\n",
    "$$ E(\\alpha_i) = 0 \\quad \\text{if} \\quad y_i \\neq P(\\alpha_i) $$\n",
    "\n",
    "If there are $e$ errors, we can construct such a polynomial of degree $e$.\n",
    "\n",
    "### The Key Equation\n",
    "\n",
    "With this definition, we can establish a key equation that holds true for **every single point**, whether it's an error or not:\n",
    "\n",
    "$$ y_i E(\\alpha_i) = P(\\alpha_i) E(\\alpha_i) \\quad \\text{for all } i=1, \\dots, n $$\n",
    "\n",
    "\n",
    "This powerful identity is easy to prove by considering two cases:\n",
    "1.  **If an error occurred at $\\alpha_i$**: By definition, $E(\\alpha_i) = 0$. The equation becomes $y_i \\cdot 0 = P(\\alpha_i) \\cdot 0$, which simplifies to $0=0$. The identity holds.\n",
    "2.  **If no error occurred at $\\alpha_i$**: In this case, we know $y_i = P(\\alpha_i)$. If we multiply both sides of this by $E(\\alpha_i)$, the equality is preserved. The identity also holds.\n",
    "\n",
    "This equation is the foundation of the algorithm. The only problem is that it involves the product of two unknowns, $P(X)$ and $E(X)$, making it a difficult quadratic problem to solve directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568da536-6109-46c2-80da-dcf470a27d42",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
